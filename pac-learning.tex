\section{PAC learning}
\label{sec:pac-learning}
\begin{enumerate}

\item ~[20 points total] A factory assembles a product that consist of
  different parts. Suppose a robot was invented to recognize whether a
  product contains all the right parts. The rules of making products
  are very simple: 1) you are free to combine any of the parts as they
  are 2) you may also cut any of the parts into two distinct pieces
  before using them. You wonder how much effort a robot would need to
  figure out the what parts are used in the product.

\begin{itemize}
\item[(a)] [5 points] Suppose that a naive robot has to recognize
  products made using only rule 1. Given $N$ available parts and each
  product made out of these constitutes a distinct hypothesis. How
  large would the hypothesis space be? Brief explain your answer.
    
Each product can be made up of $1$ to $N$ parts. When a product is made up of $1$ part, it can be from any of $N$ parts and so there are ${N \choose 1}$ ways of choosing the one part. Similarly, there are ${N \choose 2}$ ways of making a product by choosing from $2$ parts. So the total number of ways of making the product would be the sum of all these. Or in other words, the size of the hypothesis set $H$ would be

$$
\left | H \right | = \sum_{i=1}^N {N \choose i}
$$

Another way of finding $\left|H\right|$ is to consider $N$ slots and each slot tells you whether the part corresponding to the slot number has been used or not. We have $2$ choices for each slot, so for $N$ slots, we have $2^N$ choices. However, we need to not count the case where none of the parts are used (since a product cannot be made without using a part) and so we would need to subtract $1$ from the total. That is

$$
\left | H \right | = 2^N - 1
$$

\item[(b)] [5 points] Suppose that an experienced worker follows both
  rules when making a product. How large is the hypothesis space now?
  Explain.

Given a set of $N$ whole parts and $N$ broken parts, considering each part, the product may be built by 

\begin{enumerate}

\item not choosing this part
\item choosing this whole part 
\item choosing only piece 1 of this part
\item choosing only piece 2 of this part

\end{enumerate}

The assumption is that using both pieces of a broken part is the same as using the whole part. Similar to the above case, we can consider $N$ slots and each slot tells you whether the part corresponding to the slot number has been not used, used as a whole, only part 1 is used or only part 2 is used. As done above, we need to subtract $1$ from the total as this accounts for the case where no part is used. So

$$
\left | H \right | = 4^N - 1
$$

\item[(c)] [10 points] An experienced worker decides to train the
  naive robot to discern the makeup of a product by showing you the
  product samples he has assembled. There are 6 available parts. If
  the robot would like to learn any product at $0.01$ error with
  probability $99\%$, how many examples would the robot have to see?
  
In order to learn a hypothesis with $n$ dimensional features, with an error less than $\epsilon$, with a probability of $1-\delta$, we would need $m$ training samples, where $m$ is given by
$$  
m > \frac{1}{\epsilon} \left ( log_e(\left | H \right |) + log_e \left ( \frac{1}{\delta} \right ) \right )
$$

In the case given above $\left | H \right | = 4^6 -1 = 4096-1=4095$, $\epsilon=0.01$ and $\delta=1.00- 0.99=0.01$.

Substituting these values into the above in-equation, we get

\begin{equation*}
\begin{aligned}
m &> \frac{1}{0.01} \left ( log_e(4095) + log_e \left ( \frac{1}{0.01} \right ) \right )\\
&> 100 \times \left ( log_e(4095) + log_e (100) \right )\\
&> 100 \times \left ( 8.32 + 4.60 \right )\\
&>1292
\end{aligned}
\end{equation*}

This means that the robot will need to look at at least $1293$ examples to learn a hyphothesis.

\end{itemize}


\item ~[20 points, from Tom Mitchell's book] We have learned an
  expression for the number of training examples sufficient to ensure
  that every hypothesis will have true error no worse than $\epsilon$
  plus its observed training error $error_S(h)$. In particular, we
  used Hoeffding bounds to derive
\[
	m \geq  \frac{1}{2 \epsilon^2}(\ln(|H|) + \ln(1/\delta)).
\]
Derive an alternative expression for the number of training examples
sufficient to ensure that every hypothesis will have true error no
worse than $(1+\epsilon)error_S(h)$, where $0 \leq \epsilon \leq 1$. You can use general Chernoff bounds
to derive such a result.

{\bf Chernoff bounds:} Suppose $X_1, \cdots, X_m$ are the outcomes of $m$ independent coin flips (Bernoulli trials), where the probability of heads on any single trail is $Pr[X_i = 1] = p$ and the probability of tails is $Pr[X_i = 0] = 1 - p$. Define $S = X_1 + X_2 + \cdots + X_m$ to be the sum of these $m$ trials. The expected value of $S/m$ is $E[S/m] = p$. The Chernoff bounds govern the probabilty that $S/m$ will differ from $p$ by some factor $0 \leq \gamma \leq 1$.

\begin{equation}
    \begin{array}{rcl}
	Pr[S/m > (1 + \gamma) p ] \leq e^{{-mp\gamma^2}/3}\\
	Pr[S/m < (1 - \gamma) p ] \leq e^{{-mp\gamma^2}/2}
	\end{array}
\end{equation}

The empirical error is defined as $err_S(h) = \frac{\left | \left \{ f(x) \neq h(x) \right \} \right |}{m}$ and the generalization error is defined as $err_D(h) =  Pr \left [ f(x) \neq h(x) \right ]$, where $S$ is the training set, $h$ is the hypothesis that is learnt, $f(x)$ is the unknown true function, $m$ is the number of training samples and $D$ is the distribution from which samples are drawn.

Using the Chernoff bounds, we can find the expression for the probability that the expected error (or in other words the generalization error) for a single hypothesis will differ from the empirical error by some factor based on $\epsilon$, where $0 \leq \epsilon \leq 1$. This expression will be

\begin{equation*}
\begin{aligned}
Pr \left[ err_D (h) > (1+\epsilon) err_S(h) \right ] \leq e^{\frac{-mp\epsilon^2}{3}}
\end{aligned}
\end{equation*}

For the case where the hypothesis $h$ is chosen from the set of all possible hypotheses $H$, the learning algorithm will choose the hypothesis that has minimum empirical error. Using the union bound, we can say that the probability that there exists hypothesis $h \in H$, such that the expected error will differ from the empirical error, will be
\begin{equation*}
\begin{aligned}
Pr \left[\exists h; err_D (h) > (1+\epsilon) err_S(h) \right ] \leq \left | H \right | e^{\frac{-mp\epsilon^2}{3}}
\end{aligned}
\end{equation*}

In order to minimize the generalization error, we need the above probability to be less than or equal to $\delta$. This gives us

\begin{equation*}
\begin{aligned}
\left | H \right | e^{\frac{-mp\epsilon^2}{3}} &\leq \delta\\
log_e \left ( \left | H \right | \right ) - \frac{mp\epsilon^2}{3} log_e e &\leq log_e \left (\delta \right)\\
\frac{mp\epsilon^2}{3} &\geq log_e \left ( \left | H \right | \right ) - log_e \left (\delta \right)\\
m &\geq \frac{3}{p\epsilon^2} \left ( log_e \left ( \left | H \right | \right ) + log_e \left (\frac{1}{\delta} \right) \right )
\end{aligned}
\end{equation*}

\end{enumerate}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hw"
%%% End:

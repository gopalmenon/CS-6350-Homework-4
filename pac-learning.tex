\section{PAC learning}
\label{sec:pac-learning}
\begin{enumerate}

\item ~[20 points total] A factory assembles a product that consist of
  different parts. Suppose a robot was invented to recognize whether a
  product contains all the right parts. The rules of making products
  are very simple: 1) you are free to combine any of the parts as they
  are 2) you may also cut any of the parts into two distinct pieces
  before using them. You wonder how much effort a robot would need to
  figure out the what parts are used in the product.

\begin{itemize}
\item[(a)] [5 points] Suppose that a naive robot has to recognize
  products made using only rule 1. Given $N$ available parts and each
  product made out of these constitutes a distinct hypothesis. How
  large would the hypothesis space be? Brief explain your answer.
    
Since we have $N$ available parts, the product can be made with any of the following parts, which would be the hypotheses space $H$. The assumption is that we are counting a part only once to see if its been used or not.
$$
H = \{1, 2, 3,\ldots, N-1, N \}
$$

This means that the size of the hypotheses space $H$ is 

\begin{equation*}
\begin{aligned}
\left | H \right | &= 1 + 2 + 3 + \ldots + N-1 + N\\
&= \frac{N (N+1)}{2}
\end{aligned}
\end{equation*}

\item[(b)] [5 points] Suppose that an experienced worker follows both
  rules when making a product. How large is the hypothesis space now?
  Explain.

Since each of the $N$ parts can be used as is or broken into two, we have a total of $3N$  possible parts. The reason is that we have $N$ original parts and if each part is broken into two, we have $2N$ broken parts. The total sum of the number of parts is $N + 2N$ or $3N$. The product can be made out of unbroken parts or with broken parts or a combination of both. If broken parts are used, then both halves of the broken part are to be used. This means that the total number of parts from which the product can be made, is reduced to $2N$. Based on an argument similar to the one given above for the previous answer, the size of the hypothesis space $H$ will be

$$
\left | H \right | = \frac{2N(2N+1)}{2}
$$

The assumption is that we are interested in counting whole and broken parts separately.

\item[(c)] [10 points] An experienced worker decides to train the
  naive robot to discern the makeup of a product by showing you the
  product samples he has assembled. There are 6 available parts. If
  the robot would like to learn any product at $0.01$ error with
  probability $99\%$, how many examples would the robot have to see?
  
In order to learn a hypothesis with $n$ dimensional features, with an error less than $\epsilon$, with a probability of $1-\delta$, we would need $m$ training samples, where $m$ is given by
$$  
m > \frac{1}{\epsilon} \left ( log_e(\left | H \right |) + log_e \left ( \frac{1}{\delta} \right ) \right )
$$

In the case given above, since there are $2N$ different parts (since a robot can be made with broken and unbroken parts) that can be chosen to make up the product, the input space can be thought of as having $2N$ dimensions since each dimension will be a binary variable which signifies whether a part has been used or not. $\left | H \right | =\frac{2N(2N+1)}{2} = \frac{2\times 6(2 \times 6 +1)}{2} =\frac{12\times 13}{2} = 78$, $\epsilon=0.01$ and $\delta=1.00- 0.99=0.01$.

Substituting these values into the above in-equation, we get

\begin{equation*}
\begin{aligned}
m &> \frac{1}{0.01} \left ( log_e(78) + log_e \left ( \frac{1}{0.01} \right ) \right )\\
&> 100 \times \left ( log_e(78) + log_e (100) \right )\\
&> 100 \times \left ( 4.36 + 4.60 \right )\\
&>896.19
\end{aligned}
\end{equation*}

This means that the robot will need to look at at least $897$ examples to learn a hyphothesis.

\end{itemize}


\item ~[20 points, from Tom Mitchell's book] We have learned an
  expression for the number of training examples sufficient to ensure
  that every hypothesis will have true error no worse than $\epsilon$
  plus its observed training error $error_S(h)$. In particular, we
  used Hoeffding bounds to derive
\[
	m \geq  \frac{1}{2 \epsilon^2}(\ln(|H|) + \ln(1/\delta)).
\]
Derive an alternative expression for the number of training examples
sufficient to ensure that every hypothesis will have true error no
worse than $(1+\epsilon)error_S(h)$, where $0 \leq \epsilon \leq 1$. You can use general Chernoff bounds
to derive such a result.

{\bf Chernoff bounds:} Suppose $X_1, \cdots, X_m$ are the outcomes of $m$ independent coin flips (Bernoulli trials), where the probability of heads on any single trail is $Pr[X_i = 1] = p$ and the probability of tails is $Pr[X_i = 0] = 1 - p$. Define $S = X_1 + X_2 + \cdots + X_m$ to be the sum of these $m$ trials. The expected value of $S/m$ is $E[S/m] = p$. The Chernoff bounds govern the probabilty that $S/m$ will differ from $p$ by some factor $0 \leq \gamma \leq 1$.

\begin{equation}
    \begin{array}{rcl}
	Pr[S/m > (1 + \gamma) p ] \leq e^{{-mp\gamma^2}/3}\\
	Pr[S/m < (1 - \gamma) p ] \leq e^{{-mp\gamma^2}/2}
	\end{array}
\end{equation}

The empirical error is defined as $err_S(h) = \frac{\left | \left \{ f(x) \neq h(x) \right \} \right |}{m}$ and the generalization error is defined as $err_D(h) =  Pr \left [ f(x) \neq h(x) \right ]$, where $S$ is the training set, $h$ is the hypothesis that is learnt, $f(x)$ is the unknown true function, $m$ is the number of training samples and $D$ is the distribution from which samples are drawn.

Using the Chernoff bounds, we can find the expression for the probability that the expected error (or in other words the generalization error) for a single hypothesis will differ from the empirical error by some factor based on $\epsilon$, where $0 \leq \epsilon \leq 1$. This expression will be

\begin{equation*}
\begin{aligned}
Pr \left[ err_D (h) > (1+\epsilon) err_S(h) \right ] \leq e^{\frac{-mp\epsilon^2}{3}}
\end{aligned}
\end{equation*}

For the case where the hypothesis $h$ is chosen from the set of all possible hypotheses $H$, the learning algorithm will choose the hypothesis that has minimum empirical error. Using the union bound, we can say that the probability that there exists hypothesis $h \in H$, such that the expected error will differ from the empirical error, will be
\begin{equation*}
\begin{aligned}
Pr \left[\exists h; err_D (h) > (1+\epsilon) err_S(h) \right ] \leq \left | H \right | e^{\frac{-mp\epsilon^2}{3}}
\end{aligned}
\end{equation*}

In order to minimize the generalization error, we need the above probability to be less than or equal to $\delta$. This gives us

\begin{equation*}
\begin{aligned}
\left | H \right | e^{\frac{-mp\epsilon^2}{3}} &\leq \delta\\
log_e \left ( \left | H \right | \right ) - \frac{mp\epsilon^2}{3} log_e e &\leq log_e \left (\delta \right)\\
\frac{mp\epsilon^2}{3} &\geq log_e \left ( \left | H \right | \right ) - log_e \left (\delta \right)\\
m &\geq \frac{3}{p\epsilon^2} \left ( log_e \left ( \left | H \right | \right ) + log_e \left (\frac{1}{\delta} \right) \right )
\end{aligned}
\end{equation*}

\end{enumerate}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hw"
%%% End:
